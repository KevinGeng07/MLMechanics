{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e9b49e",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b33787",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca319a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c71a76",
   "metadata": {},
   "source": [
    "### 1. Generation\n",
    "\n",
    "Instate the system prompt that guides the LLMs throughout all of its responses (+ establishes user oversight).\n",
    "\n",
    "For example, the user wants to generate a merge sort implementation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a machine learning engineer tasked with teaching the user about LLM context limits.'\n",
    "            'Your task is to generate the best content possible for the user\\'s request. If the user provides critique,'\n",
    "            'respond with a revised version of your previous attempt.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c86415",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Evaluate the levels of model hallucinations and proposed their solutions. DO NOT CITE ANY PAPERS.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model='llama-3.3-70b-versatile'\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {'role': 'assistant',\n",
    "     'content': mergesort_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c79d16",
   "metadata": {},
   "source": [
    "### 2. Reflection\n",
    "\n",
    "Define another system prompt that provides oversight to the LLM's generated output. This effectively allows the LLM to analyze its response through a different lens, as ideally the critic should have a different persona (and act independently of the first LLM's input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_chat_history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are Ilya Sutskever, an experienced machine learning engineer and ex-OpenAI member. You are tasked with constructively criticizing '\n",
    "        'the provided response and citations, ensuring the model is summarizing real, credible papers and producing a response that audiences in industry can interpret without fail.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_chat_history.append(\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': mergesort_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f1151",
   "metadata": {},
   "outputs": [],
   "source": [
    "critique = client.chat.completions.create(\n",
    "    messages=reflection_chat_history,\n",
    "    model='llama-3.3-70b-versatile'\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0507179",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(critique, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a9d88",
   "metadata": {},
   "source": [
    "### 3. Generation (II)\n",
    "\n",
    "Include the critique in the chat feedback so the model is capable of updating it based on its system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': critique\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f968b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model='llama-3.3-70b-versatile'\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_markdown(essay, raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
